# Lastro.AI

> _**Lastro.AI**: Onde a complexidade da pol√≠tica monet√°ria encontra a clareza da IA, transformando relat√≥rios est√°ticos em conhecimento din√¢mico e preciso._

[![LinkedIn](https://img.shields.io/badge/%40alexcamargos-230A66C2?style=social&logo=LinkedIn&label=LinkedIn&color=white)](https://www.linkedin.com/in/alexcamargos)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Python Version](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://www.python.org/downloads/)

---

## Sobre o Projeto

O **Lastro.AI** √© um agente de Intelig√™ncia Artificial especializado projetado para decodificar e democratizar o acesso aos complexos _Relat√≥rios de Pol√≠tica Monet√°ria (RPM) e Relat√≥rios de Infla√ß√£o (RI)_ do Banco Central do Brasil.

Em um cen√°rio onde a informa√ß√£o econ√¥mica √© crucial mas frequentemente inacess√≠vel devido √† linguagem t√©cnica e formato PDF, este projeto atua como uma ponte. Por meio de **Processamento de Linguagem Natural (NLP)** e **Retrieval-Augmented Generation (RAG)**, o sistema ingere documentos oficiais, cria uma base de conhecimento vetorial e permite que pesquisadores, economistas e estudantes fa√ßam perguntas em linguagem natural, obtendo respostas fundamentadas exclusivamente nos dados oficiais.

### Principais Funcionalidades
- **Ingest√£o Automatizada**: Download e processamento autom√°tico de relat√≥rios hist√≥ricos do BACEN.
- **Busca Sem√¢ntica Avan√ßada**: Utiliza embeddings e re-ranking para encontrar trechos relevantes n√£o apenas por palavras-chave, mas pelo significado da pergunta.
- **Rastrabilidade**: Toda resposta gerada cita a fonte exata (documento e p√°gina), eliminando alucina√ß√µes.
- **Interface Conversacional**: Chatbot interativo dispon√≠vel via web (Chainlit) ou terminal.

## Instala√ß√£o e Uso

Este projeto utiliza o gerenciador de pacotes `uv` para garantir reprodutibilidade e velocidade.

### Pr√©-requisitos
- Python 3.11+
- [uv](https://github.com/astral-sh/uv) instalado.
- Chave de API da Groq (para o LLM).

### 1. Clonar o Reposit√≥rio
```bash
git clone https://github.com/alexcamargos/lastro.ai.git
cd lastro.ai
```

### 2. Configurar o Ambiente
Crie um arquivo `.env` na raiz do projeto com suas credenciais:
```env
# .env
GROQ_API_KEY="sua_chave_api_aqui"
```

### 3. Instalar Depend√™ncias
```bash
uv sync
```

### 4. Construir a Base de Conhecimento
Antes de perguntar, o sistema precisa "ler" os relat√≥rios. O comando abaixo baixa e processa documentos do √∫ltimo ano:
```bash
uv run main.py process_batch 1
```

### 5. Executar a Aplica√ß√£o
Inicie a interface web:
```bash
uv run main.py web
```
Ou fa√ßa perguntas diretas via terminal:
```bash
uv run main.py ask "Qual a proje√ß√£o de infla√ß√£o para 2025?"
```

**Exemplo de Resposta Real:**
> "No Relat√≥rio de Infla√ß√£o 2024/12 (Trecho 4), as proje√ß√µes foram revisadas indicando uma infla√ß√£o de **4,5%** para 2025... A proje√ß√£o mais recente reflete uma atualiza√ß√£o condicional √†s trajet√≥rias da taxa Selic e do c√¢mbio."
*(O sistema cruza fontes diferentes, identifica a data mais recente e justifica a resposta)*

## Demonstra√ß√£o e Exemplos

### 1. Terminai (CLI)
A interface de linha de comando √© ideal para integra√ß√µes e perguntas r√°pidas.

```bash
# Pergunta
uv run main.py ask "Por que o COPOM decidiu manter a taxa Selic?"

# Resposta do Agente (Output Real)
"""
O COPOM decidiu manter a taxa Selic em 15,00% a.a. por tr√™s raz√µes principais, conforme destacado nos relat√≥rios:

1. **Converg√™ncia da infla√ß√£o √† meta**: A manuten√ß√£o da Selic busca assegurar a converg√™ncia da infla√ß√£o para o redor da meta de 3,00% ao longo do horizonte relevante. O Comit√™ avalia que uma pol√≠tica monet√°ria contracionista por per√≠odo prolongado √© necess√°ria para consolidar o processo de desinfla√ß√£o e reancorar as expectativas de infla√ß√£o (Trechos 1, 2 e 4).

2. **Cen√°rio de incertezas e desancoragem de expectativas**: O cen√°rio atual √© marcado por expectativas desancoradas, proje√ß√µes de infla√ß√£o elevadas, resili√™ncia da atividade econ√¥mica e press√µes no mercado de trabalho. Esses fatores exigem cautela na condu√ß√£o da pol√≠tica monet√°ria, com manuten√ß√£o de juros em patamar significativamente contracionista para mitigar riscos (Trechos 1, 2 e 4).

3. **Avalia√ß√£o dos impactos acumulados**: O Comit√™ interrompeu temporariamente o ciclo de alta de juros para examinar os efeitos acumulados do ajuste anterior. A manuten√ß√£o da Selic permite observar se o n√≠vel atual √© suficiente para garantir a converg√™ncia da infla√ß√£o √† meta, mantendo flexibilidade para ajustes futuros (Trechos 1 e 4).

Al√©m disso, o COPOM enfatiza que seguir√° vigilante, podendo retomar ajustes na taxa se julgar necess√°rio, mantendo o foco na estabilidade de pre√ßos e no equil√≠brio entre desinfla√ß√£o e suaviza√ß√£o das flutua√ß√µes da atividade econ√¥mica (Trechos 1, 2 e 4).
"""
```

### 2. Interface Web (Chainlit)
Para uma experi√™ncia visual completa, a interface web oferece hist√≥rico de chat, visualiza√ß√£o de fontes e *thought process* (cadeia de pensamento).

![Demonstra√ß√£o da Interface Web](assets/demo_web.gif)
*Acima: O sistema analisando um relat√≥rio de 80 p√°ginas em segundos para responder sobre o 'Hiato do Produto'.*

## Stack Tecnol√≥gico

O projeto foi constru√≠do sobre um stack moderno de Python, priorizando performance e tipagem est√°tica.

*   **Linguagem**: Python 3.12 (com Type Hints rigorosos).
*   **Orquestra√ß√£o de LLM**: [LangChain](https://www.langchain.com/) para constru√ß√£o das cadeias de RAG.
*   **Interface Web**: [Chainlit](https://chainlit.io/) para uma experi√™ncia de chat r√°pida e reativa.
*   **Banco Vetorial**: [FAISS](https://github.com/facebookresearch/faiss) (Facebook AI Similarity Search) para busca de alta performance.
*   **CLI**: [Fire](https://github.com/google/python-fire) para cria√ß√£o autom√°tica de interfaces de linha de comando.
*   **Gerenciamento de Depend√™ncias**: [uv](https://github.com/astral-sh/uv) (substituto moderno ao Pip/Poetry).

## Decis√µes Arquiteturais e T√©cnicas

Como um projeto focado em **Data Science e Engenharia de IA**, cada componente foi escolhido para resolver um problema espec√≠fico de escalabilidade ou precis√£o.

### 1. Retrieval-Augmented Generation (RAG)
Optou-se por uma arquitetura RAG em vez de fine-tuning.
*   **Por qu√™?** Relat√≥rios do BACEN mudam trimestralmente. RAG permite atualizar o conhecimento do modelo instantaneamente apenas adicionando novos documentos ao √≠ndice, sem o custo e tempo de re-treinar o modelo. Al√©m disso, RAG mitiga alucina√ß√µes ao for√ßar o modelo a usar o contexto recuperado.

### 2. Modelo de LLM: Llama 3 / Mixtral (via Groq)
O sistema utiliza a API da Groq para infer√™ncia.
*   **Por qu√™?** A Groq oferece a menor lat√™ncia de infer√™ncia do mercado (LPU), essencial para uma experi√™ncia de chat fluida. Os modelos Open Source (Llama 3 70B ou Mixtral 8x7B) demonstraram capacidade de racioc√≠nio compar√°vel ao GPT-4 para tarefas de sumariza√ß√£o e extra√ß√£o em portugu√™s, com custo zero ou muito reduzido.

### 3. Embeddings e Re-Ranking (Dois Est√°gios)
A recupera√ß√£o de informa√ß√£o √© feita em duas etapas para maximizar a precis√£o (Hit Rate).
*   **Est√°gio 1 (Recall)**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`. Um modelo bi-encoder r√°pido que busca os 20 documentos topologicamente mais pr√≥ximos.
*   **Est√°gio 2 (Precision)**: `cross-encoder/mmarco-mMiniLMv2-L12-H384-v1`. Um modelo Cross-Encoder que re-avalia e pontua os documentos recuperados.
*   **Por qu√™?** Busca vetorial simples falha em nuances lingu√≠sticas. O Cross-Encoder atua como um "juiz" detalhista, garantindo que o contexto entregue ao LLM seja extremamente relevante, dobrando a precis√£o do sistema.

## Estudo de Caso: Problema, Solu√ß√£o e Aprendizado

### Problema
Profissionais que dependem de dados do Banco Central perdem horas lendo PDFs de 80 p√°ginas em busca de par√°grafos espec√≠ficos sobre "metas de infla√ß√£o" ou "cen√°rio externo". A busca por palavras-chave (Ctrl+F) √© falha pois o vocabul√°rio t√©cnico varia ("infla√ß√£o" vs "IPCA" vs "n√≠vel de pre√ßos").

### Solu√ß√£o
Desenvolvimento de um pipeline de ETL n√£o estruturado que converte PDFs em vetores sem√¢nticos. O sistema n√£o busca por palavras, mas por **conceitos**. Se o usu√°rio pergunta sobre "custo de vida", o sistema recupera trechos sobre "IPCA", mesmo que a palavra "custo" n√£o apare√ßa, gra√ßas aos embeddings multilingues.

### Aprendizado
*   **Qualidade dos Dados > Modelo**: A maior melhoria de performance n√£o veio de trocar o LLM, mas de melhorar o "chunking" (segmenta√ß√£o) dos PDFs e limpar cabe√ßalhos/rodap√©s que polu√≠am a busca.
*   **Avalia√ß√£o Quantitativa**: Implementar m√©tricas como "Hit Rate" foi essencial para sair do "acho que melhorou" para "melhorou 40%". A engenharia de IA exige rigor cient√≠fico tanto quanto engenharia de software tradicional.

---

## Autor

Feito com ‚ù§Ô∏è por [Alexsander Lopes Camargos](https://github.com/alexcamargos) üëã Entre em contato!

[![GitHub](https://img.shields.io/badge/-AlexCamargos-1ca0f1?style=flat-square&labelColor=1ca0f1&logo=github&logoColor=white&link=https://github.com/alexcamargos)](https://github.com/alexcamargos)
[![Twitter Badge](https://img.shields.io/badge/-@alcamargos-1ca0f1?style=flat-square&labelColor=1ca0f1&logo=twitter&logoColor=white&link=https://twitter.com/alcamargos)](https://twitter.com/alcamargos)
[![Linkedin Badge](https://img.shields.io/badge/-alexcamargos-1ca0f1?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/alexcamargos/)](https://www.linkedin.com/in/alexcamargos/)
[![Gmail Badge](https://img.shields.io/badge/-alcamargos@vivaldi.net-1ca0f1?style=flat-square&labelColor=1ca0f1&logo=Gmail&logoColor=white&link=mailto:alcamargos@vivaldi.net)](mailto:alcamargos@vivaldi.net)

## Licen√ßa
Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.
